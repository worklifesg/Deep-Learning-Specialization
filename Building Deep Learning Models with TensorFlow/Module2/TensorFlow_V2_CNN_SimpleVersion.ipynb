{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Example: Convolutional Neural Network using TensorFlow (2.3.0) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convolution Neural Network</h3>\n",
    "\n",
    "![CNN](https://raw.githubusercontent.com/worklifesg/Deep-Learning-Specialization/master/images/CNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MNIST Dataset</h3>\n",
    "\n",
    "The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. \n",
    "\n",
    "![MNIST](https://raw.githubusercontent.com/worklifesg/Deep-Learning-Specialization/master/images/MnistExamples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 1: Import Libraries</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 2: Parameters</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNSIT dataset parameters (No need of num_input and dropout as in version 1.15)\n",
    "num_classes=10\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "learning_rate=0.001\n",
    "num_steps=500\n",
    "batch_size=128\n",
    "display_step=10\n",
    "\n",
    "# Network parameters (No tf graph input through placeholder as in version 1.15)\n",
    "\n",
    "conv1_filters=32 # first layer\n",
    "conv2_filters=64 # second layer\n",
    "fc1_units=1024 #neurons in fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step4: MNIST dataset from Keras dataset </h3>\n",
    "\n",
    "<h4> In version 1.15, we used tensorflow.examples.tutorials.mnist packaage, which is no more available in version 2, so it is better to use datasets from keras </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)= mnist.load_data() # get training and testing data\n",
    "\n",
    "#x is the data for images and y is the classes and labels\n",
    "\n",
    "x_train,x_test = np.array(x_train,np.float32),np.array(x_test,np.float32) #First convert the data to float32\n",
    "x_train,x_test = x_train/255,x_test/255 # Normalize the image value from [0,255] to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and Batch Data\n",
    "# -- In version 1.15, we used batch size in sess.run nad used mnist.train.next_batch(batch_size) but in 2.3.0 we will\n",
    "# use tf.data for training data\n",
    "\n",
    "train_data=tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "train_data=train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 5: Model creation, Loss Function, Accuracy Metric, Optimizer </h3>\n",
    "\n",
    "<h4> Version 1.15 vs 2.3.0: </h4>\n",
    "\n",
    "<h4> 1. For class definition, the process is same but in version 1.15, weights and biases are used to define layers, flatten,fully connected layer, dropout and output whereas in 2.3.0, number of filters and kernel size is used. In dropout now, rate is used instead of keep_prob from previous version.</h4>\n",
    "\n",
    "<h4> 2. In loss function, previous version used 'softmax_cross_entropy_with_logits', which has been replaced from nn Module package function 'sparse_softmax_cross_entropy_with_logits'.</h4>\n",
    "\n",
    "<h4> 3. In optimizer, there is no change </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_total(Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(conv_total,self).__init__() #allow us to create objects from nn module without initializing them explicitly\n",
    "        \n",
    "        self.conv1=layers.Conv2D(32,kernel_size=5,activation=tf.nn.relu) #1st Convolution Layer [32 filters, kernal =5]\n",
    "        self.maxpool1=layers.MaxPool2D(2,strides=2) # down-sampling with kernal size 2 and strides of 2\n",
    "        \n",
    "        self.conv2=layers.Conv2D(64,kernel_size=3,activation=tf.nn.relu) #1st Convolution Layer [64 filters, kernal =3]\n",
    "        self.maxpool2=layers.MaxPool2D(2,strides=2) # down-sampling with kernal size 2 and strides of 2\n",
    "        \n",
    "        self.flatten=layers.Flatten() # Flatten data to 1D vector for fully connected layer\n",
    "        \n",
    "        self.fc1=layers.Dense(1024) #Fully connected layer with 1024 neurons\n",
    "        \n",
    "        self.dropout=layers.Dropout(rate=0.5) #Dropout only for true training else not applied\n",
    "        \n",
    "        self.out=layers.Dense(num_classes) #output layer (classes/labels)\n",
    "    \n",
    "    def call(self,x,is_training=False): #For prediction\n",
    "        x=tf.reshape(x,[-1,28,28,1])\n",
    "        x=self.conv1(x)\n",
    "        x=self.maxpool1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.maxpool2(x)\n",
    "        x=self.flatten(x)\n",
    "        x=self.fc1(x)\n",
    "        x=self.dropout(x,training=is_training)\n",
    "        x=self.out(x)\n",
    "        \n",
    "        if not is_training:\n",
    "            x=tf.nn.softmax(x) #apply softmax when not training\n",
    "        return x\n",
    "\n",
    "## Build Neural Network\n",
    "        \n",
    "conv_total=conv_total()              \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "def cross_entropy_loss(x,y):\n",
    "    y=tf.cast(y,tf.int64)  # convert labels to int64\n",
    "    loss=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=x) #softmax to logits and compute cross entropy\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Accuracy\n",
    "\n",
    "def accuracy(y_pred,y_true):\n",
    "    correct_prediction=tf.equal(tf.argmax(y_pred,1),tf.cast(y_true,tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction,tf.float32),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 6: Optimization process </h3>\n",
    "\n",
    "<h4> In this version we wrap computation using GradientTape and update the variables and compute gradients </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(x,y):\n",
    "    with tf.GradientTape() as g: # computing optimization process as session run\n",
    "        pred=conv_total(x,is_training=True) #using prediction funtion in conv_total()\n",
    "        loss=cross_entropy_loss(pred,y) #compute loss\n",
    "        \n",
    "    train_var=conv_total.trainable_variables # variables update\n",
    "    grad=g.gradient(loss,train_var) #compute gradients\n",
    "    \n",
    "    optimizer.apply_gradients(zip(grad,train_var)) #update weights and biases following gradients\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 7: Train Model </h3>\n",
    "\n",
    "<h4> In this version, we use .take instead of .next_batch</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10, loss: 1.901503, accuracy: 0.757812\n",
      "step: 20, loss: 1.604883, accuracy: 0.898438\n",
      "step: 30, loss: 1.579184, accuracy: 0.929688\n",
      "step: 40, loss: 1.619967, accuracy: 0.890625\n",
      "step: 50, loss: 1.567867, accuracy: 0.953125\n",
      "step: 60, loss: 1.523846, accuracy: 0.968750\n",
      "step: 70, loss: 1.513477, accuracy: 0.968750\n",
      "step: 80, loss: 1.535113, accuracy: 0.945312\n",
      "step: 90, loss: 1.540618, accuracy: 0.945312\n",
      "step: 100, loss: 1.528899, accuracy: 0.968750\n",
      "step: 110, loss: 1.508346, accuracy: 0.984375\n",
      "step: 120, loss: 1.499631, accuracy: 0.992188\n",
      "step: 130, loss: 1.524543, accuracy: 0.960938\n",
      "step: 140, loss: 1.490476, accuracy: 1.000000\n",
      "step: 150, loss: 1.498449, accuracy: 0.976562\n",
      "step: 160, loss: 1.482981, accuracy: 0.992188\n",
      "step: 170, loss: 1.499913, accuracy: 0.984375\n",
      "step: 180, loss: 1.491768, accuracy: 0.984375\n",
      "step: 190, loss: 1.506130, accuracy: 0.976562\n",
      "step: 200, loss: 1.497489, accuracy: 0.976562\n",
      "step: 210, loss: 1.486004, accuracy: 0.992188\n",
      "step: 220, loss: 1.499008, accuracy: 0.968750\n",
      "step: 230, loss: 1.489142, accuracy: 0.984375\n",
      "step: 240, loss: 1.484454, accuracy: 0.992188\n",
      "step: 250, loss: 1.504526, accuracy: 0.960938\n",
      "step: 260, loss: 1.481170, accuracy: 0.992188\n",
      "step: 270, loss: 1.510024, accuracy: 0.976562\n",
      "step: 280, loss: 1.484020, accuracy: 1.000000\n",
      "step: 290, loss: 1.483141, accuracy: 0.984375\n",
      "step: 300, loss: 1.477141, accuracy: 0.984375\n",
      "step: 310, loss: 1.483796, accuracy: 0.992188\n",
      "step: 320, loss: 1.488901, accuracy: 0.984375\n",
      "step: 330, loss: 1.480859, accuracy: 0.992188\n",
      "step: 340, loss: 1.491372, accuracy: 0.984375\n",
      "step: 350, loss: 1.490661, accuracy: 0.984375\n",
      "step: 360, loss: 1.507828, accuracy: 0.976562\n",
      "step: 370, loss: 1.508785, accuracy: 0.953125\n",
      "step: 380, loss: 1.482421, accuracy: 0.992188\n",
      "step: 390, loss: 1.516403, accuracy: 0.945312\n",
      "step: 400, loss: 1.495668, accuracy: 0.968750\n",
      "step: 410, loss: 1.480940, accuracy: 0.992188\n",
      "step: 420, loss: 1.486876, accuracy: 0.992188\n",
      "step: 430, loss: 1.477121, accuracy: 0.992188\n",
      "step: 440, loss: 1.484576, accuracy: 0.992188\n",
      "step: 450, loss: 1.487782, accuracy: 0.976562\n",
      "step: 460, loss: 1.474575, accuracy: 1.000000\n",
      "step: 470, loss: 1.465593, accuracy: 1.000000\n",
      "step: 480, loss: 1.470773, accuracy: 1.000000\n",
      "step: 490, loss: 1.476413, accuracy: 0.992188\n",
      "step: 500, loss: 1.482004, accuracy: 0.992188\n",
      "Test Accuracy: 0.980600\n"
     ]
    }
   ],
   "source": [
    "for step, (batch_x,batch_y) in enumerate(train_data.take(num_steps),1):\n",
    "    optimization(batch_x,batch_y)\n",
    "    \n",
    "    if step % display_step==0:\n",
    "        pred=conv_total(batch_x)\n",
    "        loss=cross_entropy_loss(pred,batch_y)\n",
    "        acc=accuracy(pred,batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
    "\n",
    "# Test model on validation set (accuracy)\n",
    "pred = conv_total(x_test)\n",
    "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
